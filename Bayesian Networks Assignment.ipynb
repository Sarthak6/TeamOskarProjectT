{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 16ML Week 5 Assignment: Bayesian Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1: Introduction to the Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week in class, we covered Bayesian Networks - probabilistic graph based modelling tools that rely on Bayesian inference for certain probability computations/queries. They aim to model conditional independences and dependences between a set of events (and thus causation) via their structure which is discussed later on. As we saw in class, having. domain knowledge is particularly useful when it comes to modelling Bayes Nets - and that should be clearer by the end of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course so far, we have covered a lot of the theory behind Bayesian Networks - whether it be construction, inference, parameter estimation, or structure learning. We have also seen how large Bayes Nets can get - see the picture below which shows a Bayes Net of certain diseases and their effects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Pretty Large Bayes Net of certain diseases and their effects\n",
    "<img src=\"images/0_large_bayes_net.jpg\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of how large Bayes Nets can get, we need a way to program Bayes Nets and do all our inference and prediction calculations using a computer. For this, will be using the well known Python library [Pomegranate](https://pomegranate.readthedocs.io/en/latest/BayesianNetwork.html#fitting). You are welcome to and encouraged to explore the library and its uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is divided into various segments - in each segment you will use Pomegranate to reinforce the Bayes Net theory you learnt and learn how to program this theory. In the case of certain algorithms learnt in class such as those for parameter estimation and structure learning, you will also get first hand experience with how to implement and test out/compare these algorithms against the library version. The reason behind this is that while it is easy (and important) to be able to call the right library functions, it is also important to be able to know what is going on behind the scenes and implement this in case you may need to make minor modifications to well known algorithms (which is quite often the case in research and the industry)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the following sections in this notebook:\n",
    "\n",
    "<b>0. Getting Started:</b> The current section - which involves a brief introduction to the assignment and setting up of the required libraries\n",
    "\n",
    "<b>1. Bayes Net Construction and Basic Inference:</b> This section will involve learning how to use Pomegranate to construct basic Bayesian Networks and how inference can be carried out with them.\n",
    "\n",
    "<b>2. Parameter Estimation and Involved Algorithms:</b> In this section you will learn how Pomegranate can be used to invoke techniques seen in lecture such as MLE to learn certain parameters. You will also code your own version of an algorithm to do this.\n",
    "\n",
    "<b>3. Structure Construction and Involved Algorithms:</b> This section is similar in structure to the previous one. You will learn how Pomegranate can be used to invoke Bayes Net structure construction algorithms and you will also implement your own version of these algorithms.\n",
    "\n",
    "<b>4. Takeaways and Learning Objectives Satisfied</b>\n",
    "\n",
    "<b>5. References</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2: Setting up Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, we will be using the Pomegranate library for this function. Follow the instructions on [this](https://pomegranate.readthedocs.io/en/latest/install.html) page to install Pomegranate. You may need to install [pip](https://pip.pypa.io/en/stable/installing/) in the process. Once you have successfully installed Pomegranate, run the following cell which imports Pomegranate and other required libraries needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Imports\n",
    "import pomegranate\n",
    "import pygraphviz\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from pomegranate import *\n",
    "from sklearn.datasets import load_digits\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is imperative for the rest of this assignment that the last cell ran without any bugs. If any library could not be imported (not just pomegranate), make sure to install it. Google is your friend - \"How to install libraryXYZ?\" <b>Do not move on unless the last cell runs without any errors!<b/>\n",
    "    \n",
    "NOTE: To install [pygraphviz](https://pygraphviz.github.io/documentation/latest/install.html), you may need to install [graphviz](https://graphviz.org/download/). Download any tools/dependencies you may need using Google Search. If you are having trouble contact sarthakarora@berkeley.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Bayes Net Construction and Basic Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the definition of a Bayes Net from lecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1_bayes_net_definition.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see to construct a Bayes Net we will need to build a DAG with conditional probability tables (CPT's) for each node - which defines the probability of values a node can take on conditioned on values all of that nodes parents can take on. In this section we will assume that we know the structure of the Bayes Net and know the CPT's. Let's look at the following Bayes Net. Follow the code below the picture to see how we can construct this Bayes Net using Pomegranate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Bayesian Network Representing if John or Mary call the police depending on whether an alarm rings which depends can depend on a Burglary or Earthquake\n",
    "<img src=\"images/1_alarm_network.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Constructing the Bayes Net\n",
    "#### Thoroughly read through each code cell below to see how Pomegranate can be used to construct the Bayes Net above. Fill in the blanks where applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start of by building the Conditional Probability Tables for each node. Most have been done.\n",
    "# Use the CPT's filled out for you as an example to fill out the CPT's that have been left blank.\n",
    "\n",
    "# Burglary Node\n",
    "possible_values_burglary = [\"Burglary\", \"No Burglary\"] # For us to remind ourselves what values this node can take on\n",
    "burglary_cpt = DiscreteDistribution({\"Burglary\": 0.001, \"No Burglary\": 0.999}) # Model nodes with no parents as discrete distributions\n",
    "\n",
    "# Earthquake Node\n",
    "possible_values_earthquake = [\"Earthquake\", \"No Earthquake\"] \n",
    "earthquake_cpt = ...\n",
    "\n",
    "#Alarm Node\n",
    "possible_alarm_values = [\"Alarm\", \"No Alarm\"]\n",
    "# Since this node is conditioned on parent/s, we will use the CPT class instead of the DiscreteDistribution class\n",
    "alarm_cpt = ConditionalProbabilityTable(\n",
    "        [[ 'Earthquake', 'Burglary', 'Alarm', 0.95 ],\n",
    "         [ 'Earthquake', 'No Burglary', 'Alarm', 0.29 ],\n",
    "         [ 'No Earthquake', 'Burglary', 'Alarm', 0.94 ],\n",
    "         [ 'No Earthquake', 'No Burglary', 'Alarm', 0.001 ],\n",
    "         ...,\n",
    "         ...,\n",
    "         ...,\n",
    "         ...], [earthquake_cpt, burglary_cpt]) # Need to pass in Parent CPT's in same order\n",
    "# NOTE: Each row corresponds to a CPT entry. The last column (3rd in this case) of each entry\n",
    "# is the value of the node this CPT corresponds to. The preceding columns are values of the parent nodes\n",
    "# of this alarm node (Earthquake and Burglary). The order of the columns of parents has to be the same\n",
    "# as the order of the parent CPT's passed into the ConditionalProbabilityTable constructor which in the above \n",
    "# case is [earthquake_cpt, burglary_cpt].\n",
    "\n",
    "# John Node\n",
    "possible_john_values = [\"John Calls\", \"John Doesn't Call\"]\n",
    "john_cpt = ConditionalProbabilityTable(\n",
    "        [[ \"Alarm\", \"John Calls\", 0.9 ],\n",
    "         [ \"No Alarm\", \"John Calls\", 0.05 ],\n",
    "         [ \"Alarm\", \"John Doesn't Call\", 0.1 ],\n",
    "         [ \"No Alarm\", \"John Doesn't Call\", 0.95 ]], [alarm_cpt]) \n",
    "\n",
    "# Mary Node\n",
    "possible_mary_values = [\"Mary Calls\", \"Mary Doesn't Call\"]\n",
    "mary_cpt = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we formalise each Node and its CPT as a state object\n",
    "burglary_state = State(burglary_cpt, name=\"burglary\")\n",
    "earthquake_state = State(earthquake_cpt, name=\"earthquake\")\n",
    "alarm_state = State(alarm_cpt, name=\"alarm\")\n",
    "john_state = ...\n",
    "mary_state = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we need to create a Bayesian Network Model, with a name, and add our states to this model\n",
    "model = BayesianNetwork(\"Alarm Bayes Net\")\n",
    "model.add_states(burglary_state, earthquake_state, alarm_state, john_state, mary_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will add edges to our model representing parent-child relationships between states\n",
    "# The call to do this looks like model.add_edge(parent, child) and edges look like parent --> child\n",
    "model.add_edge(burglary_state, alarm_state)\n",
    "model.add_edge(earthquake_state, alarm_state)\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we must bake our model to finish preparing it\n",
    "model.bake()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! If the cells above did not error, you now know how to build a Bayes Net using Pomegranate! Run the cells below to see how the Bayes Net has been stored by Pomegranate. You do not need to fully understand the output of the following two cells - it is just if you are curious! (If the cells above did error, check your answers and retry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.graph # A graph representation of the Bayes Net as stored by Pomegranate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.edges # How the states, their distributions, and the edges are stored by Pomegranate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Prediction and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how to build a Bayes Net in Pomegranate, we can move on to something more interesting, prediction and inference. This includes computing answers to certain queries such as finding joint and conditional probabilities that are not directly readable from the CPT's originally given to us in the Bayesian Network. We leverage conditional independence relationships in Bayes Nets to do this inference and as a reminder from lecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Recap From Lecture\n",
    "<img src=\"images/1_inference.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have learnt how to do inference the hard mechanical way in lecture and on other assignments - however it is here on a computer where libraries such as Pomegranate shine and make your life easy. Let's say you're interested in finding P(Burglary, Earthquake, Alarm, John Calls, Mary Calls). This involves multiplying a whole bunch of values to get the joint distribution as you have learnt, but all you have to do in pomegranate is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P('Burglary', 'Earthquake', 'Alarm', \"John Calls\", \"Mary Calls\")\n",
    "model.probability([['Burglary', 'Earthquake', 'Alarm', \"John Calls\", \"Mary Calls\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, here are a few more values one may be interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P('Burglary', 'No Earthquake', 'Alarm', \"John Calls\", \"Mary Doesn't Call\")\n",
    "model.probability([['Burglary', 'No Earthquake', 'Alarm', \"John Calls\", \"Mary Doesn't Call\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P('Burglary', 'No Earthquake', 'Alarm', \"John Doesn't Call\", \"Mary Doesn't Call\")\n",
    "model.probability([['Burglary', 'No Earthquake', 'Alarm', \"John Doesn't Call\", \"Mary Doesn't Call\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P('No Burglary', 'No Earthquake', 'No Alarm', \"John Doesn't Call\", \"Mary Doesn't Call\")\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P('No Burglary', 'No Earthquake', 'No Alarm', \"John Calls\", \"Mary Doesn't Call\")\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P('Burglary', 'No Earthquake', 'No Alarm', \"John Calls\", \"Mary Doesn't Call\")\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want the joint over only certain nodes, use None for nodes you want to sum out\n",
    "# P('No Burglary', 'No Alarm', \"John Doesn't Call\", \"Mary Doesn't Call\")\n",
    "model.probability([['No Burglary', None, 'No Alarm', \"John Doesn't Call\", \"Mary Doesn't Call\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P('No Burglary', 'No Earthquake', 'No Alarm', \"Mary Doesn't Call\")\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to computing certain joint distributions as above, the true power of inference is seen when given any set of observed variables, including no observations, Bayesian networks can make predictions for all other variables.\n",
    "\n",
    "As per pomegranate, \"We can run inference using the predict_proba method and passing in a dictionary of values, where the key is the name of the state and the value is the observed value for that state. If we don't supply any values, we get the marginal of the graph, which is just the frequency of each value for each variable over an infinite number of randomly drawn samples from the graph.\"\n",
    "\n",
    "Let us now see the marginal distribution for each node. <b>Is this what you expected? Explain why or why not.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the marginal distribution of each node\n",
    "model.predict_proba({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Your Observations Here: </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how we are given five discrete distribution objects corresponding to the marginal distribution at each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very obvious type of inference we may be interested in is finding certain conditional probabiity distributions after having observed certain occurences. For example, let us say we know John called. In this case, we may be interested in the marginal distribution at each node knowing that John Called. Specifically we may be interested in knowing P(Alarm | John Called). Follow the code below to learn how to get updated marginal distributions once you have observed evidence. It is important to note the order in which your list is structured in the code below should be the same order as in which your states were added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a reminder, our states were added in the order [burglary_state, earthquake_state, alarm_state, john_state, mary_state]\n",
    "# This order has to be preserved when using the predict_proba({}) function\n",
    "# For example if we only observed John called, and wanted the updated marginals.\n",
    "model.predict_proba([[None, None, None, 'John Calls', None]]) # Observed John called, None for each other node as have no observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how this gave you the marginal at each node given the evidence John called. Specifically P(Alarm | John called) was 0.04 which is significantly greater than P(Alarm) = 0.002. Does this make sense? \n",
    "\n",
    "Now imagine we observe that Mary called as well. <b>Find each of the updated marginal distributions. What is P(Alarm | John called, Mary Called). Does this value make sense? Why?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both John and Mary called\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Your Answer: </b>\n",
    "\n",
    "You now have been walked through how to do inference on your own - hopefully you can see the use of a library such as Pomegranate in performing quick inference as opposed to doing all these calculations manually.\n",
    "\n",
    "Continue on to the next subsection to implement an entire Bayes Net construction and inference pipeline on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Do It Yourself: The Monty Hall Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will construct a Bayes Net on your own and run inference on it to solve a common problem statement: The Monty Hall Porblem. The problem is as follows:\n",
    "\n",
    "There are three closed doors: Door one, two and three. Behind two of the doors lie a goat and behind one of the doors lies a brand new car. At the start of the game, a contestant can pick any of the closed doors (note: even after the contestant picks a door the door stays closed.) Now since there are 2 goats and 3 doors in total, regardless of the door the contestant chooses, there is at least one unchosen door that has a goat behind it. Monty (who knows what is behinf each closed door), the game show host, opens a door the contestant didn't choose and reveals a goat. The question asked to the contestant now is: In order to win the car, do you want to stick with the door you originally chose or switch doors to the other unopened door. You win the car if the car is behind the door you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monty Hall Setup. Assume you first picked Door 1 and then Monty revealed Door 3 has a goat behind it. Do you switch to Door 2 or do you stick with Door 1? \n",
    "<img src=\"images/1_monty_hall_setup.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it may seem that there is a 50-50 percent chance of winning regardless of whether you stick to your original door or switch doors. However, you will show via Bayesian Inference that there is an advantage in one strategy over the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to model this problem as a Bayes Net. For this Bayes Net, assume you have three states - \"Guest Choice\", \"Prize Door\", \"Monty Opens\". <b> Each state can take on three values, \"1\", \"2\", \"3\" </b> corresponding to what door the contestant chooses, what door the prize is behind, and what door Monty chooses to open. Using this information and the problem statement draw out what you think your Bayes net might look like in the following cell. One example is \"Guest Choice\" --> \"Prize Door\" --> \"Monty Opens\".\n",
    "\n",
    "Hint: The example answer is wrong. What states affect what? Can the guest initially see which door has the prize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... # Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to fill out the Conditional Probability Tables for each node. Use the following information to fill out the conditional probability tables in the cell below: \"The door the guest initially chooses and the door the prize is behind are completely random processes across the three doors, but the door which Monty opens is dependent on both the door the guest chooses (it cannot be the door the guest chooses), and the door the prize is behind (it cannot be the door with the prize behind it). If Monty can open 2 doors, he opens each of them with equal probability.\" Remember, <b> each state can take on three values, \"1\", \"2\", \"3\". </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the Conditional Probability Tables\n",
    "\n",
    "# Completely random\n",
    "guest = ...\n",
    "\n",
    "# Completely random\n",
    "prize = ...\n",
    "\n",
    "# Dependent on guest and the prize. \n",
    "# Hint: This CPT should have 27 entries. Some entries will have 0 probability\n",
    "monty = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, formalise the states, create a model and add the states to the model, add the edges, and bake the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating States\n",
    "guest_state = ...\n",
    "prize_state = ...\n",
    "monty_state = ...\n",
    "\n",
    "# Create the Model\n",
    "monty_model = BayesianNetwork(\"Monty Hall Problem\")\n",
    "\n",
    "# Add the states to the model\n",
    "monty_model.add_states(...)\n",
    "\n",
    "# Add the required edges\n",
    "monty_model.add_edge(...)\n",
    "monty_model.add_edge(...)\n",
    "\n",
    "# Bake the model\n",
    "monty_model.bake()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above steps ran without error, you should probably have a fully functioning Bayes Net representing the Monty Hall problem. As a sanity check, run the following two cells. They should output 0.111 and 0 respectively. Try to convince yourself of why this is this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monty_model.probability([['1', '2', '3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monty_model.probability([['1', '3', '3']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the more interesting part: inference. Remember, the question we wanted to answer was whether to stick with the same door or to change door. We will answer this question using inference.\n",
    "\n",
    "To go about doing this, run inference on the following scenario. Imagine the guest picked door 1. Now Monty has to open either door 2 or 3 and reveal a goat. Imagine he opened door 3 and revealed a goat. Run inference to find the probability of the prize being behind each door given these observations and report them.\n",
    "\n",
    "As a reminder, the states were added in the following order: (guest_state, prize_state, monty_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evidence is guest_state = 1, and monty_state = 3\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Your Observation. Should we switch doors or not? What is the corresponding probability for each?: </b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Although we assumed which door gets picked and which door gets opened, the same argument holds for all combinations of what door gets picked and what door Monty opens due to symmetry. Monty will always be open one door revealing a goat due to the setup of the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see through Bayesian Inference that the optimal way to win the car is to switch doors. If you are interested, try to find probabilities of the car being behind a given door if there are k doors, you choose 1, and Monty opens k-2 doors all revealing goats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Parameter Estimation and Involved Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, for both the examples we were given enough information to explicitly write out the structure of our Bayes Net and the associated Conditional Probability Tables. However, in the real world we will not be given the structure and our CPT's. While certain domain knowledge may help us draw out what we think may be the Bayes Net structure, there is no way to 'intuitively' know the values that will go into our conditional probability tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: The Dyspnea Bayes Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we can use data and machine learning in a very naive way. Consider the well known Bayes Net below, which  tries to model what may effect if one has dyspnoea (shortness of breath).\n",
    "\n",
    "Note: The Bayes Net is used as a very common modelling example in the medical world and has no intention to hurt anyone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dyspnoea Bayes Net\n",
    "<img src=\"images/2_dyspnoea_net.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we can model the structure of this Bayes Net using domain knowledge, however we do not have any of the conditional probability table entries. Luckily for us, like in most machine learning tasks, we have a bunch of data in the dataframe below. Each row corresponds to the record for one person and each column corresponds to the state value for that given state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyspnea_data = pd.read_csv(\"data/dyspnea_final.csv\")\n",
    "dyspnea_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer these sanity questions: \n",
    "\n",
    "1. What does each row correspond to? How many rows are there? Ans: \n",
    "\n",
    "2. What does each column correspond to? What is special about the number of columns? Ans: \n",
    "\n",
    "3. What is the domain of each node? Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the main problem. We want to somehow use this data to fill in the entries in our conditional probability table. We can formulate each entry in the CPT's as a parameter and then our goal becomes to use the data to best predict our parameters. To do this, we will estimate parameters based on the maximum likelihood estimation (MLE) proccedure covered in class and the next sub-section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Parameter Estimation using MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic behind MLE, as a quick recap of lecture is that given some parameter theta and some data, choose the parameter such that it maximizes the likelihood of the observed data. Here, our observed data is the table you saw above and a parameter is an entry in a CPT. Thus we have multiple parameters, equal to the number of CPT entries we have to fill. We can find each parameter one by one using MLE, and as a quick recap:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE\n",
    "<img src=\"images/2_mle.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple words, MLE can be extracted by counting from our data. Essentially, for each variable, you need consider only that column of data and the columns corresponding to that variables parents. If it is a univariate distribution, then the maximum likelihood estimate is just the count of each symbol divided by the number of samples in the data. If it is a multivariate distribution, it ends up being the probability of each symbol in the variable of interest given the combination of symbols in the parents. For example, if you wanted the MLE estimate for P(Smoking = Yes), you would count the total number of rows in your data where smoking was yes and divide that by the total number of rows in your data. Similarly, if you wanted the MLE estimate for P(Dyspnea = No | Bronchitis = Yes), you would count the total number of rows where Dyspnea is No and Bronchitis is Yes and divide that by the number of rows where Bronchitis is Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pomegranate, to do this MLE estimation, you will have to build and bake the entire Bayesian Model as before but with each CPT entry as 0. After this, call fit on the model and pass the data in and the model will calculate the MLE CPT parameters and modify the CPT entries for you. Walk through the following cells to see how this is done filling in code where needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CPT's with 0 for each entry\n",
    "# Keep in mind the domain of each Node is [Yes, No]\n",
    "\n",
    "asia = DiscreteDistribution( { 'Yes' : 0, 'No' : 0 } )\n",
    "\n",
    "tuberculosis = ConditionalProbabilityTable(\n",
    "    [[ 'Yes', 'Yes', 0 ],\n",
    "     [ 'Yes', 'No', 0 ],\n",
    "     [ 'No', 'Yes', 0 ],\n",
    "     [ 'No', 'No', 0 ]], [asia])\n",
    "\n",
    "smoking = DiscreteDistribution( { 'Yes' : 0, 'No' : 0 } )\n",
    "\n",
    "lung = ConditionalProbabilityTable(\n",
    "    [[ 'Yes', 'Yes', 0 ],\n",
    "     [ 'Yes', 'No', 0 ],\n",
    "     [ 'No', 'Yes', 0 ],\n",
    "     [ 'No', 'No', 0 ]], [smoking] )\n",
    "\n",
    "bronchitis = ConditionalProbabilityTable(\n",
    "    [[ 'Yes', 'Yes', 0 ],\n",
    "     [ 'Yes', 'No', 0 ],\n",
    "     [ 'No', 'Yes', 0 ],\n",
    "     [ 'No', 'No', 0 ]], [smoking] )\n",
    "\n",
    "tuberculosis_or_cancer = ConditionalProbabilityTable(\n",
    "    [[ 'Yes', 'Yes', 'Yes', 0 ],\n",
    "     [ 'Yes', 'Yes', 'No', 0 ],\n",
    "     [ 'Yes', 'No', 'Yes', 0 ],\n",
    "     [ 'Yes', 'No', 'No', 0 ],\n",
    "     [ 'No', 'Yes', 'Yes', 0 ],\n",
    "     [ 'No', 'Yes', 'No', 0 ],\n",
    "     [ 'No', 'No', 'Yes', 0 ],\n",
    "     [ 'No', 'No', 'No', 0 ]], [tuberculosis, lung] )\n",
    "\n",
    "xray = ConditionalProbabilityTable(\n",
    "    [[ 'Yes', 'Yes', 0 ],\n",
    "     [ 'Yes', 'No', 0 ],\n",
    "     [ 'No', 'Yes', 0 ],\n",
    "     [ 'No', 'No', 0 ]], [tuberculosis_or_cancer] )\n",
    "\n",
    "dyspnea = ConditionalProbabilityTable(\n",
    "    [[ 'Yes', 'Yes', 'Yes', 0 ],\n",
    "     [ 'Yes', 'Yes', 'No', 0 ],\n",
    "     [ 'Yes', 'No', 'Yes', 0 ],\n",
    "     [ 'Yes', 'No', 'No', 0 ],\n",
    "     [ 'No', 'Yes', 'Yes', 0 ],\n",
    "     [ 'No', 'Yes', 'No', 0 ],\n",
    "     [ 'No', 'No', 'Yes', 0 ],\n",
    "     [ 'No', 'No', 'No', 0 ]], [tuberculosis_or_cancer, bronchitis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create states\n",
    "s0 = State( asia, name=\"asia\" )\n",
    "s1 = State( tuberculosis, name=\"tuberculosis\" )\n",
    "s2 = State( smoking, name=\"smoker\" )\n",
    "s3 = State( lung, name=\"cancer\" )\n",
    "s4 = State( bronchitis, name=\"bronchitis\" )\n",
    "s5 = State( tuberculosis_or_cancer, name=\"either\" )\n",
    "s6 = State( xray, name=\"xray\" )\n",
    "s7 = State( dyspnea, name='dyspnea' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, add the states\n",
    "dyspnea_network = BayesianNetwork( \"Dyspnea Network\" )\n",
    "dyspnea_network.add_nodes(s0, s1, s2, s3, s4, s5, s6, s7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all the edges, and bake \n",
    "dyspnea_network.add_edge( s0, s1 )\n",
    "dyspnea_network.add_edge( s1, s5 )\n",
    "dyspnea_network.add_edge( s2, s3 )\n",
    "dyspnea_network.add_edge( s2, s4 )\n",
    "dyspnea_network.add_edge( s3, s5 )\n",
    "dyspnea_network.add_edge( s5, s6 )\n",
    "dyspnea_network.add_edge( s5, s7 )\n",
    "dyspnea_network.add_edge( s4, s7 )\n",
    "\n",
    "dyspnea_network.bake()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the new part. We will call .fit() on the model and pass in our dyspnea_data DataFrame. Observe how the ouput of the call shows the updated fitted conditional probability tables and answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate parameters from Data using MLE\n",
    "dyspnea_network.fit(dyspnea_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: What is the Probability of Someone visiting Asia\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: What is the probability of someone not having tuberculosis if they did not visit Asia?\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: Write a line or two of code to find the following probability:\n",
    "# P(\"asia\" = No,\"tuberculosis\" = Yes, \"smoker\" = Yes, \"cancer\" = No, \"bronchitis\" = Yes, \"Either\" = Yes, \"xray\" = No, 'dyspnea' = Yes))\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4: Write a line or two of code to find the following probability:\n",
    "# P(tuberculosis\" = Yes, \"smoker\" = Yes, \"cancer\" = No, \"bronchitis\" = Yes, \"Either\" = Yes, \"xray\" = No))\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5: Write a line or two of code to find the following marginal probability:\n",
    "# P(\"asia\" | \"tuberculosis\" = Yes, \"smoker\" = Yes, \"cancer\" = No, \"bronchitis\" = Yes, \"Either\" = Yes, \"xray\" = No, 'dyspnea' = Yes))\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now know how to use Pomegranate to compute parameters using Data and the MLE method. However, you also know what is going on in the MLE method. Therefore, the next section implement involves implementing your own function that does exactly what the .fit function does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing a Naive Version of MLE Yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function below, which takes in a dataframe corresponding to your data, and your query in the form of two dictionaries - the query_dictionary and the evidence_dictionary. This is perhaps best explained through an example. \n",
    "\n",
    "Consider you wanted to find the ML estimate of P(Dyspnea = Yes | Bronchitis = Yes, Either = No), then your data argument would be dyspnea_data, your query_dictionary would be {\"Dyspnea\": \"Yes\"} and your evidence dictionary would be {\"Bronchitis\" : \"Yes\", \"Either\": \"No\"}. Keep in mind your dictionary keys have to be actual columns in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_ml_estimate(data, query_dictionary, evidence_dictionary):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to find the MLE of a parameter embodied by the query_dictionary and evidence_dictionary and given a\n",
    "    dataframe called data. If you wanted to find the ML estimate of P(Dyspnea = Yes | Bronchitis = Yes, Either = No), \n",
    "    then your data argument would be dyspnea_data, your query_dictionary would be {\"Dyspnea\": \"Yes\"} \n",
    "    and your evidence dictionary would be {\"Bronchitis\" : \"Yes\", \"Either\": \"No\"}. \n",
    "    Keep in mind your dictionary keys have to be actual columns in your dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct a 'target_df' that corresponds to a DF where all rows match the evidence in evidence_dictionary\n",
    "    target_df = data\n",
    "    for (state, observation) in evidence_dictionary.items():\n",
    "        assert state in target_df.columns, \"Tried to condition on a state not present in your Bayes Net/Data\"\n",
    "        ...\n",
    "    \n",
    "    # Find the number of rows which are consistent with all your evidence\n",
    "    total_consistent_rows_evidence = ...\n",
    "    if total_consistent_rows_evidence == 0:\n",
    "        print(\"Found no Data consistent with your evidence. Consider using smoothing. Returning 0 for now\")\n",
    "        return ...\n",
    "    \n",
    "    # Construct a 'query_df' that corresponds to a DF where all rows are consistent with your query and evidence\n",
    "    query_df = target_df\n",
    "    for (state, observation) in query_dictionary.items():\n",
    "        assert state in query_df.columns, \"Tried to query a state not present in your Bayes Net/Data\"\n",
    "        ...\n",
    "    \n",
    "    # Find the number of rows which are consistent with all your evidence and queries\n",
    "    total_consistent_rows_evidence_and_queries = ...\n",
    "    if total_consistent_rows_evidence_and_queries == 0:\n",
    "        print(\"Found no Data consistent with your evidence and query. Consider using smoothing. Returning 0 for now\")\n",
    "        return ...\n",
    "    \n",
    "    # Using your computed quantities, return your ML Estimate for the parameter\n",
    "    ml_estimate = ...\n",
    "    assert ml_estimate >= 0 and ml_estimate <= 1, \"Probability has to be between 0 and 1. Check for errors\"\n",
    "    return ml_estimate\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your implemented function to compute the following parameters. If you have a correct implementation, you should get the exact same values as when you used the .fit() function. Use the results from before as a sanity check of your own implementation. Here is the dataframe for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyspnea_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dyspnea_data #Setting up for following questions/function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: What is the Probability of Someone visiting Asia\n",
    "query_dictionary = ...\n",
    "evidence_dictionary = ...\n",
    "parameter_ml_estimate(data, query_dictionary, evidence_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: What is the probability of someone not having tuberculosis if they did not visit Asia?\n",
    "query_dictionary = ...\n",
    "evidence_dictionary = ...\n",
    "parameter_ml_estimate(data, query_dictionary, evidence_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: What is P(Dyspnea = Yes | Bronchitis = Yes, Either = No)\n",
    "# What does the answer to this question tell you?\n",
    "query_dictionary = ...\n",
    "evidence_dictionary = ...\n",
    "parameter_ml_estimate(data, query_dictionary, evidence_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing a Naive Version of MLE With Laplace Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code till now was correct, the last answer should have returned 0 with an assertion message of \"Found no Data consistent with your evidence\". This should make sense. With the ML estimate we, first count the number of rows matching the evidence in our data. The error message means that there were no rows in our dataset matching the evidence which is why we returned 0.\n",
    "\n",
    "In a sense, we do not want 0 probilities - as it is a very strong claim that something can never happen. This is happening as our data does not have a certain record. Intuitively, we are relying too much to and overfitting to our data. Just beause a record did not occur in our 'train' data it does not mean it can not occur in the future - and in such a case we should never return 0 probability.\n",
    "\n",
    "To generalise our parameter estimations and avoid 'the problem of zero', we will use Laplace Smoothing. As you learned in lecture, with Laplace smoothing we introduce an additive hyperparameter 'k' and assume each possible value in a queried distribution occurs k more times than it actually does. This prevents something from having a probability = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace Smoothing\n",
    "<img src=\"images/2_Laplace.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Implement the function below, which allows smoothing and thus takes in two additional parameters - k and domain_size, which corresponds to the number of domains the query variable can take. You are encouraged to copy your code from the previous function wherever possible.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_ml_estimate_smoothed(data, query_dictionary, evidence_dictionary, domain_size, k=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to find the MLE of a parameter embodied by the query_dictionary and evidence_dictionary and given a\n",
    "    dataframe called data. Enables Laplace Smoothing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct a 'target_df' that corresponds to a DF where all rows match the evidence in evidence_dictionary\n",
    "    target_df = data\n",
    "    for (state, observation) in evidence_dictionary.items():\n",
    "        assert state in target_df.columns, \"Tried to condition on a state not present in your Bayes Net/Data\"\n",
    "        target_df = ...\n",
    "    \n",
    "    # Find the number of rows which are consistent with all your evidence and include the smoothing factor \n",
    "    total_consistent_rows_evidence = ...\n",
    "    total_consistent_rows_evidence_smoothed = ...\n",
    "    if total_consistent_rows_evidence_smoothed == 0:\n",
    "        print(\"Found no Data consistent with your evidence. Consider using smoothing. Returning 0 for now\")\n",
    "        return ...\n",
    "    \n",
    "    # Construct a 'query_df' that corresponds to a DF where all rows are consistent with your query and evidence\n",
    "    query_df = target_df\n",
    "    for (state, observation) in query_dictionary.items():\n",
    "        assert state in query_df.columns, \"Tried to query a state not present in your Bayes Net/Data\"\n",
    "        query_df = ...\n",
    "    \n",
    "    # Find the number of rows which are consistent with all your evidence and queries and include the smoothing factor\n",
    "    total_consistent_rows_evidence_and_queries = ...\n",
    "    total_consistent_rows_evidence_and_queries_smoothed = ...\n",
    "    if total_consistent_rows_evidence_and_queries_smoothed == 0:\n",
    "        print(\"Found no Data consistent with your evidence. Consider using smoothing. Returning 0 for now\")\n",
    "        return ...\n",
    "    \n",
    "    # Using your computed quantities, return your ML Estimate for the parameter\n",
    "    ml_estimate = ...\n",
    "    assert ml_estimate >= 0 and ml_estimate <= 1, \"Probability has to be between 0 and 1. Check for errors\"\n",
    "    return ml_estimate\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now that you have implemented Laplace Smoothing, run the query that gave you 0 probability with the following values of k: 0, 1, 5. What do you notice? Do this by setting the required variables in the next cell, running it, and using the slider outputted to vary the value of k. Record your observations </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Variables and Slider\n",
    "domain_size = ...\n",
    "data = ...\n",
    "query_dictionary = ...\n",
    "evidence_dictionary = ...\n",
    "\n",
    "# You do not need to read or understand this next line\n",
    "interact(parameter_ml_estimate_smoothed, k=widgets.IntSlider(min=0, max=10, step=1, value=2), data=fixed(data), query_dictionary=fixed(query_dictionary), evidence_dictionary=fixed(evidence_dictionary), domain_size=fixed(domain_size));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 0. Use the slider above and record your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1. Use the slider above and record your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 5. Use the slider above and record your observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Your Observations: </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the probability of someone not having tuberculosis if they did not visit Asia for k = 0, 1, 5, 10, 100, 1000. Then run the cell that plots a graph for you, and write out what you notice and understand. Follow the same process with the slider as in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Variables and Slider\n",
    "domain_size = ...\n",
    "data = ...\n",
    "query_dictionary = ...\n",
    "evidence_dictionary = ...\n",
    "\n",
    "# You do not need to read or understand this next line\n",
    "interact(parameter_ml_estimate_smoothed, k=widgets.IntSlider(min=0, max=1000, step=1, value=2, layout=Layout(width='1000px')), data=fixed(data), query_dictionary=fixed(query_dictionary), evidence_dictionary=fixed(evidence_dictionary), domain_size=fixed(domain_size));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 0. Use the slider above and record your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1. Use the slider above and record your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 5. Use the slider above and record your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 10. Use the slider above and record your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 100. Use the slider above and record your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1000. Use the slider above and record your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just Run this Cell and write what you notice in the cell below.\n",
    "# This cell may take a while to run\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(121)\n",
    "plt.title(\"The effect of k on P(Tuberculosis = No | Asia = No)\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel(\"P(Tuberculosis = No | Asia = No)\", fontsize=14)\n",
    "plt.xlabel(\"k\", fontsize=14)\n",
    "\n",
    "xs = np.linspace(1, 20000, 500)\n",
    "ys = np.array([parameter_ml_estimate_smoothed(data, query_dictionary, evidence_dictionary, domain_size, my_k) for my_k in xs])\n",
    "plt.plot(xs, ys, c='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question:</b> What do you observe? Why do you think this is happening? Where will the curve level out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, in this section you learnt how to implement Maximum Likelihood based Parameter estimation using data when you may not have the CPT entries from before. You also learnt how to automatically do this in Pomegranate. Lastly, you saw how ML estimates may overfit to data and thus we can use Laplace smoothing to generalise our model. There are various other algorithms for smoothing and parameter estimation such as MAP, the EM algorithm - which you are encouraged to check out but are out of scope for this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Structure Construction and Involved Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 1, we saw how to construct a Bayes Net and run inference using it given the structure and the completed CPT's.\n",
    "\n",
    "In section 2, we assumed we still had the Bayes Net structure but did not have the CPT entries. Instead we had a bunch of data and we learnt how to use this data to estimate the CPT entires and then run inference as normal.\n",
    "\n",
    "In this section, we still have our data, but now we will assume we have not been given the Bayes Net structure as well (and lets assume we have no prior/domain knowledge about the structure). Now the goal becomes to try to learn the Bayes Net structure. Once we have done this, we can apply techniques learnt in section 1 and 2 to do anything we normally would with a Bayes Net. Thus, we will formulate the problem in this section as Bayes Net Structure Learning, or BNSL for short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: BNSL Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many algorithms for BSNL and they usually fall under the following fields:\n",
    "\n",
    "1. Score and Search Based: Keep in mind from our data we will always know the associated states in a Bayes Net. Also, remember that a Bayes Net structure is a DAG. This algorithm searches over the space of all possible (DAGs) and identifies the one that minimizes some objective function. Typical objective functions attempt to balance the log probability of the data given the model (the likelihood). As you may expect, the runtime of this algorithm becomes exponential in the number of variables and thus this algorithm is very unfeasible.\n",
    "\n",
    "2. Constraint Learning: This typically involves calculating some measure of correlation or co-occurrence to identify an undirected backbone of edges that may exist, and then prune these edges until a DAG is reached. There is no probabilistic interpretation of this type of algrithms so do not spend too much time trying to understand these for now.\n",
    "\n",
    "3. Approximate Algorithms: The key idea here is that we want to make score and search quick while maintaining its accuracy. Approximate algorithms develop heuristics for score and search to quickly find good structures in a reasonable amount of time. This class of algorithms include the Chow Liu algorithm and the hill climbing algorithm you have seen in lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomegranate Currently supports four BNSL algorithms. You do not need to know their details for now as tht is beyond the scope of this course (although we will discuss Chow Liu Trees later), but these are the algorithms:\n",
    "\n",
    "1. Chow Liu Trees - Add each possible edge in the DAG with an associated weight (representing the \"quality\" of that edge), and then build a MST using thes weights. Discussed in more detail later on.\n",
    "\n",
    "2. Greedy - This uses the Score and Search with A* whose heuristics are out of scope.\n",
    "\n",
    "3. Exact DP - This uses the Score and Search with Dynamic Programing whose details are out of scope.\n",
    "\n",
    "4. Exact Length Based - Uses the score and search and restricts the number of edges to get a bit of speedup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Using and Comparing the Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pomegranate, to use any of these algorithms, we use the .from_samples function where we pass in our dataset and the  algorithm name we want to use. As an example, lets try to recreate the structure of the Dyspnea Bayes Net we used before. As a reminder, here is the Bayes Net (assume we don't know this) and the data. Follow the code cells below to see how we can use the data to predict structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dyspnoea Bayes Net\n",
    "<img src=\"images/2_dyspnoea_net.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data as a reminder\n",
    "dyspnea_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Chow Liu Trees Algorithm to predict Structure\n",
    "model = BayesianNetwork.from_samples(dyspnea_data, algorithm='chow-liu')\n",
    "model.plot()\n",
    "# Note: State 0 corresponds to the 0th column in your data, State 1 to the 1st and so on\n",
    "# Thus: [0-Asia, 1-Tuberculosis, 2-Smoke, 3-Lung Cancer, 4-Bronchitis, 5-Either, 6-X-Ray, 7-Dyspnea]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observations and Comparisons to True Structure:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Exact Length Based Algorithm to predict Structure\n",
    "model = BayesianNetwork.from_samples(dyspnea_data, algorithm='exact')\n",
    "model.plot()\n",
    "# Note: State 0 corresponds to the 0th column in your data, State 1 to the 1st and so on\n",
    "# Thus: [0-Asia, 1-Tuberculosis, 2-Smoke, 3-Lung Cancer, 4-Bronchitis, 5-Either, 6-X-Ray, 7-Dyspnea]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observations and Comparisons to True Structure:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using A* Greedy Algorithm to predict Structure\n",
    "model = BayesianNetwork.from_samples(dyspnea_data, algorithm='greedy')\n",
    "model.plot()\n",
    "# Note: State 0 corresponds to the 0th column in your data, State 1 to the 1st and so on\n",
    "# Thus: [0-Asia, 1-Tuberculosis, 2-Smoke, 3-Lung Cancer, 4-Bronchitis, 5-Either, 6-X-Ray, 7-Dyspnea]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observations and Comparisons to True Structure:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Exact-DP Algorithm to predict Structure\n",
    "model = BayesianNetwork.from_samples(dyspnea_data, algorithm='exact-dp')\n",
    "model.plot()\n",
    "# Note: State 0 corresponds to the 0th column in your data, State 1 to the 1st and so on\n",
    "# Thus: [0-Asia, 1-Tuberculosis, 2-Smoke, 3-Lung Cancer, 4-Bronchitis, 5-Either, 6-X-Ray, 7-Dyspnea]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observations and Comparisons to True Structure:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Large Bayes Nets To Compare Performance of the Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following common and much bigger example of a Bayes Net. In this setup we have a picture of a number and the picture is divided into 64 pixels. Each pixel has a brightness value and each pixel depends on the number the picture was representing. Thus, think of this Bayes Net of having 65 states (1 for each pixel and 1 for the number in the picture). \n",
    "\n",
    "The structure of the Bayes Net is not important, what is important is that this Bayes Net is much larger than the ones we have dealt with. We want to see how the different algorithms recover the structure of the Bayes Net as we consider more and more states. Run the following cell which generates data, and tries to recover the structure using the different BNSL algorithms while varying the number of states in the Bayes Net.\n",
    "\n",
    "Note: This cell may take more than 5 minutes to run and you do not need to understand any of the code - just the graphs that are outputted from it. This code is mainly from the Pomegranate Source Repository and is credited to the writer of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU DO NOT NEED TO UNDERSTAND THIS\n",
    "X, _ = load_digits(10, True)\n",
    "X = X > numpy.mean(X)\n",
    "\n",
    "t1, t2, t3, t4 = [], [], [], []\n",
    "p1, p2, p3, p4 = [], [], [], []\n",
    "n_vars = range(8, 19)\n",
    "\n",
    "for i in n_vars:\n",
    "    X_ = X[:,:i]\n",
    "\n",
    "    tic = time.time()\n",
    "    model = BayesianNetwork.from_samples(X_, algorithm='exact-dp') # << BNSL done here!\n",
    "    t1.append(time.time() - tic)\n",
    "    p1.append(model.log_probability(X_).sum())\n",
    "\n",
    "    tic = time.time()\n",
    "    model = BayesianNetwork.from_samples(X_, algorithm='exact')\n",
    "    t2.append(time.time() - tic)\n",
    "    p2.append(model.log_probability(X_).sum())\n",
    "\n",
    "    tic = time.time()\n",
    "    model = BayesianNetwork.from_samples(X_, algorithm='greedy')\n",
    "    t3.append(time.time() - tic)\n",
    "    p3.append(model.log_probability(X_).sum())\n",
    "\n",
    "    tic = time.time()\n",
    "    model = BayesianNetwork.from_samples(X_, algorithm='chow-liu')\n",
    "    t4.append(time.time() - tic)\n",
    "    p4.append(model.log_probability(X_).sum())\n",
    "    \n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Time to Learn Structure\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel(\"Time (s)\", fontsize=14)\n",
    "plt.xlabel(\"Variables\", fontsize=14)\n",
    "plt.plot(n_vars, t1, c='c', label=\"Exact DP\")\n",
    "plt.plot(n_vars, t2, c='m', label=\"Exact\")\n",
    "plt.plot(n_vars, t3, c='g', label=\"Greedy\")\n",
    "plt.plot(n_vars, t4, c='r', label=\"Chow-Liu\")\n",
    "plt.legend(fontsize=14, loc=2)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"$P(D|M)$ with Resulting Model\", fontsize=14)\n",
    "plt.xlabel(\"Variables\", fontsize=14)\n",
    "plt.ylabel(\"logp\", fontsize=14)\n",
    "plt.plot(n_vars, p1, c='c', label=\"Exact DP\")\n",
    "plt.plot(n_vars, p2, c='m', label=\"Exact\")\n",
    "plt.plot(n_vars, p3, c='g', label=\"Greedy\")\n",
    "plt.plot(n_vars, p4, c='r', label=\"Chow-Liu\")\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question</b>: Look at the graphs outputted. The axes on the first graph should make it pretty clear what it is trying to convey. For the second graph, we have the log of P(D|M) on the y-axis. P(D|M) is a measure of how well the built model explains the data and higher values for P(D|M) mean that the model has done a better job of explaining the data. Using these 2 graphs, compare and contrast the 4 algorithms available in Pomegranate. Note: The purple and cyan lines superimpose on the right plot as they produce the same graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer</b>: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Implementing the Chow Liu Tree Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final part of this assignment, you will implement the Chow Liu Tree algorithm you saw in action above and in lecture. We have chosen to implement this algorithm as it is intuitive, relatively simple, and performs quickly and with decent quality for most Bayes Nets. After you implement the alogrithm, you will also get a chance to run it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the algorithm always returns a tree and this is how it functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chow Liu Algorithm\n",
    "<img src=\"images/3_chow_liu_1.png\" width=500 height=500 />\n",
    "<img src=\"images/3_chow_liu_2.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note: </b> For this class you do not need to know how to find the Maximum Weight Spanning Tree. This will be done for you when you implement the algorithm. You just need to understand that it is the tree such that the sum of the weights of the edges in the tree is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convince yourself that this is the same as giving each weight -I and then finding a Minimum Weight Spanning Tree (MWST). You should use this strategy as we will let you call a function that directly builds a mimimum spanning tree for you. Also remind yourself from lecture how a higher value for I corresponds to a greater need for an edge between the two nodes in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all the information you have till nwow, implement the chow_liu_tree function - which takes in an argument called data - which is your dataframe. You will have to also fill in the helper functions and these will start to make more and more sense to you as you dive into the code. Lastly, for this problem we will be using the Networkx library for ease of implementation. In fact, the Pomegranate library uses NetworkX as well. All NetworkX syntax has been provided where needed and you are encouraged to search up relevant documentation if you feel the need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joint_marginal(data, u, v):\n",
    "    \"\"\"\n",
    "    Return the marginal distribution for the u'th and v'th indexed features of data\n",
    "    Return a dictionary of form {(u_value1, v_value1): probability, (u_value2, v_value2: probability}\n",
    "    \"\"\"\n",
    "    total_rows = ...\n",
    "    marginal = {}\n",
    "    for i in range(len(data)): # Go through all rows\n",
    "        value_of_u_in_row_i = ... # Find value of u'th feature in i'th row\n",
    "        value_of_v_in_row_i = ... # Find value of v'th feature in i'th row\n",
    "        marginal[(value_of_u_in_row_i, value_of_v_in_row_i)] = ... # Increment associated probability\n",
    "    return marginal\n",
    "\n",
    "def get_marginal(data, u):\n",
    "    \"\"\"\n",
    "    Return the marginal distribution for the u'th indexed feature of data\n",
    "    Return a dictionary of form {value1: probability, value2: probability}\n",
    "    \"\"\"\n",
    "    total_rows = ...\n",
    "    marginal = {}\n",
    "    for i in range(len(data)): # Go through all rows\n",
    "        value_of_u_in_row_i = ... # Find value of u'th feature in i'th row\n",
    "        marginal[value_of_u_in_row_i] = ... # Increment associated probability\n",
    "    return marginal\n",
    "\n",
    "def calculate_edge_weight(data, u, v):\n",
    "    \"\"\"\n",
    "    Data is your data\n",
    "    u and v are the indices of the features for which we are calculating \"weight\"\n",
    "    \"\"\"   \n",
    "    I = 0\n",
    "    \n",
    "    u_marginal = ... # Get marginal distributions required by the algorithm\n",
    "    v_marginal = ...\n",
    "    joint_marginal = ...\n",
    "\n",
    "    for u_value, u_value_probability in u_marginal.items():\n",
    "        for v_value, v_value_probability in v_marginal.items():\n",
    "            if ...: # Go through the marginals and the joint as in algorithm\n",
    "                # Do the Calculation Involved in finding weights\n",
    "                I += ...\n",
    "    return I\n",
    "\n",
    "def chow_liu_tree(data):\n",
    "    \"\"\"\n",
    "    Approximate a BN structure using the Chow Liu Algorithm and observation data.\n",
    "    data argument is of type pd.DataFrame\n",
    "    Uses networkx library and returned stucture is in the form of a networkx graph.\n",
    "    \"\"\"\n",
    "    G = nx.Graph() # Initialises a networkx graph called G\n",
    "    number_of_states = ...\n",
    "    \n",
    "    for i in range(number_of_states):\n",
    "        G.add_node(i) # Add a node for each state. State i corresponds to the ith column in data\n",
    "        \n",
    "    for v in range(...):\n",
    "        for u in range(...): # For each possible edge\n",
    "            I = .. # Calculate weight of edge\n",
    "            G.add_edge(u, v, weight= ...) # Add edge to G with negative weight\n",
    "            \n",
    "\n",
    "    return nx.minimum_spanning_tree(G) # Return MWST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a correct implementation, running the next cell will approximate the structure of the Dyspnea BN we saw before using the Chow Liu Algorithm. Running the next cell will output the edges in the structure your algorithm returned. Compare this with the Pomegranate implemetation. Do you get the same thing? If so, well done - you implemented the algorithm correctly!\n",
    "\n",
    "Note: Although your implementation of the algorithm may have all the same edges as the Pomegranate implementation, the direction of a couple of the edges may be different when the two versions are compared. This is okay, as each node only has one parent and regardless of the edge direction, you can end up encoding the same likelihood as mutual information is symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Our Implementation\n",
    "built_structure = chow_liu_tree(dyspnea_data)\n",
    "built_structure.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pomegranate Implementation \n",
    "model = BayesianNetwork.from_samples(dyspnea_data, algorithm='chow-liu')\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed your version ran a lot slower - do not worry about that.\n",
    "\n",
    "The important thing is you now know how to construct a BN structure using Pomegranate and different algoithms and you have managed to implement one of those algorithms yourself!\n",
    "\n",
    "This marks the end of section 3 and the end of the assignment. Section 4 goes over learning objectives this assignment  was designed to fulfill and section 5 is references used for the making of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Takeaways and Learning Objectives Satisfied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you learnt a lot about constructing Bayes Net in Python using Pomegranate, using these Bayes Nets for inference, parameter estimation and structure estimation. Along the way, you got experience with how to do these different tasks and you also implemented a few of them yourself from scratch. You should realise by now that libraries such as Pomegranate are pretty useful when it comes to Bayes Nets - this is because Bayes Nets can pretty quickly get large and running inference, likelihood calculations can get arduous very easily. Moreover, just given some data, you are now equipped to first find a structure, then do parameter estimation, and any inference you may need to do. You also worked with real life datasets and Bayes Nets. Here is a more detailed explanation of the learning objectives achieved in each section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Section 1</b>: This section taught you how Pomegranate can be used to build a Bayes Net whose structure and CPT's you know. You saw how once you have 'baked' your model, you can answer any query you may have by running inference using Pomegranate's well defined and easy to use interface. Furthermore, in this section you got practice in converting a problem description to a Bayes Net with associated CPT's. You then ran inference to understand the dynamics of the Monty Hall problem. At the end of the section, you should have got enough practice on constructing a Bayes Net and knowing how to answer any queries using it. You answered conitional probability, marginal, and joint queries easily using Pomegranate.\n",
    "\n",
    "\n",
    "\n",
    "<b>Section 2</b>: This section taught you how Pomegranate can be used to estimate the entries of a CPT in a Bayes Net whose structure you know and for which you have been given observed data for each variable in the net. You saw how once you have 'baked' your model with each CPT entry as 0, you can 'fit' to the data you have observed and this will automatically populate the CPT's for you using the ML Parameter estimation procedure. Furthermore, you saw that once this is done you can run inference as normal. In fact, you understood the ML Estimation algorithm and implemented it on your own and saw it performed exactly like the Pomegranate version. Lastly, you learnt about overfitting to observed data and the problem of zero- and to solve this you understood Laplace smoothing and added this functionality to your custom built ML Parameter Estimation algorithm. Overall, you got a good grasp of implementing and understanding parameter observation having observed some data.\n",
    "\n",
    "\n",
    "\n",
    "<b>Section 3</b>: This section taught you how Pomegranate can be used to estimate the structure itself of  Bayes Net for which you have been given observed data for each variable in the net. You saw how there are various algorithms that can help us out with this process and you compared and contrasted the four algorithms that Pomegranate implements for this purpose - understanding the tradeoff between speed and 'goodness'. We looked at the Chow Liu Tree algorithm in detail and you succcessfully implemented this from scratch and saw it gave you results similar to the Pomegranate version. All in all, at the end of this section you were well aware of how to go from just observed data to finding a structure (assuming no prior domain knowledge), to finding parameters, to running vanilla inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://pomegranate.readthedocs.io/en/latest/BayesianNetwork.html#prediction]\n",
    "\n",
    "[https://www.bnlearn.com/about/slides/slides-ibm16.pdf]\n",
    "\n",
    "[https://arxiv.org/ftp/arxiv/papers/1304/1304.2736.pdf]\n",
    "\n",
    "[http://www.cs.cmu.edu/~guestrin/Class/10701/recitations/r10/11152007chowliu.pdf]\n",
    "\n",
    "[https://www.bnlearn.com/bnrepository/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

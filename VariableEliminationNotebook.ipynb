{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Elimination\n",
    "Before you get to using the `pomegranate` library, a bit of review of the variable elimination algorithm from lecture (notes). Below you'll find simple `RandomVariable` and `BayesNet` classes. The `BayesNet` class has an _abstract_ method called `marginal_probability`, which takes a list of nodes and values and returns the probability of that set of values turning up. If you don't know or don't remember what an abstract method is, it is a method left without an implementation in a _parent_ class that should be _overridden_ in a _child_ class. There is another class called `BruteForceBayesNet` that implements a naÃ¯ve calculation of the marginal probabilities as mentioned in the lecture notes so that you can see the intended functionality of these methods.\n",
    "\n",
    "Your job for this section is to implement the class `VariableEliminationBayesNet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We've included a RandomVariable class for you. You can use this to sample\n",
    "from random variables if you'd like. In this notebook, we use the\n",
    "random_clone() method for generating Bayes nets, and we RandomVariable\n",
    "objects as the nodes of our networks.\n",
    "\"\"\"\n",
    "class RandomVariable(object):\n",
    "    \"\"\"\n",
    "    Represents a discrete random variable.\n",
    "    \"\"\"\n",
    "    def __init__(self, probs_map):\n",
    "        # probs_map = {value1:probability1, ...}\n",
    "        if type(probs_map) == dict:\n",
    "            self.probs_map = probs_map\n",
    "            self.values = list(probs_map.keys())\n",
    "            self.rescale()\n",
    "        # probs_map = [value1, value2, ...]\n",
    "        else:\n",
    "            self.values = probs_map\n",
    "            self.probs_map = {}\n",
    "            # generate the probabilities randomly (useful later)\n",
    "            self.randomize()\n",
    "            \n",
    "    # if you want to visualize a random variable at any point, this prints\n",
    "    # the values that the variable takes and their probabilities. Note, however,\n",
    "    # that the BayesNet class does not automatically update the probabilities\n",
    "    # of its constituent RandomVariables based on its conditional probabilities\n",
    "    # (you can use BruteForceBayesNet's marginal_probability function if you want)\n",
    "    # to determine an individual variable's marginal distribution.\n",
    "    def __str__(self):\n",
    "        s = \"       Random Variable       \"\n",
    "        s += \"\\n \" + \"-\" * 27\n",
    "        s += \"\\n|%-12s | %12s|\" % (\"Value\", \"Probability\")\n",
    "        s += \"\\n \" + \"-\" * 27\n",
    "        for val in self.values:\n",
    "            s += \"\\n|%-12s | %12f|\" % (str(val), self.probs_map[val])\n",
    "        s += \"\\n \" + \"-\" * 27\n",
    "        return s\n",
    "    def __repr__(self):\n",
    "        return \"RandomVariable(\" + str(self.probs_map) + \")\"\n",
    "    def randomize(self):\n",
    "        \"\"\"Randomize the probabilities of each outcome.\"\"\"\n",
    "        for val in self.values:\n",
    "            self.probs_map[val] = np.random.uniform()\n",
    "        self.rescale()\n",
    "    def rescale(self):\n",
    "        \"\"\"Rescale if a bad distribution is passed or after randomizing.\"\"\"\n",
    "        total_prob = float(sum(self.probs_map.values()))\n",
    "        if total_prob == 0:\n",
    "            self.randomize()\n",
    "        for key in self.values:\n",
    "            self.probs_map[key] /= total_prob\n",
    "    def draw(self, c=3):\n",
    "        \"\"\"Sample from this RandomVariable. You might never use this.\"\"\"\n",
    "        assert c > 0, \"Out of tries\"\n",
    "        x = np.random.uniform()\n",
    "        s = 0\n",
    "        for val in self.values:\n",
    "            s += self.probs_map[val]\n",
    "            if x < s:\n",
    "                return val\n",
    "        return self.draw(c-1)\n",
    "    def random_clone(self):\n",
    "        \"\"\"\n",
    "        Create a RandomVariable with the same value set but randomized\n",
    "        probabilities. This is helpful for creating random Bayes' nets.\n",
    "        \"\"\"\n",
    "        return RandomVariable(self.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class contains just the information required to define a Bayesian network.\n",
    "That is, it stores a list of nodes, each of which is a `RandomVariable`. It also\n",
    "stores conditional probability matrices for each of the nodes; this is represented\n",
    "by the variable `self.cond_probs`, which has _numbers_ (the index of each node)\n",
    "as its keys and `dict`s as values. Each `dict` value has as its keys `tuple`s of\n",
    "values (note - not `list`s - that would throw an error) of the parent variables\n",
    "of the node (i.e. the nodes with arrows from themselves pointing to the given node).\n",
    "We store a list of edges (each edge is a `tuple`) in `self.edges` if you would like\n",
    "to use it, and we store the children (the endpoints of arrows that start at a node)\n",
    "and parents (the starting points of arrows that end at a node) of each node in `self.children`\n",
    "and `self.parents`, respectively.\n",
    "\n",
    "The method you should certainly note is the `marginal_probability` method. This method,\n",
    "as noted above, is _abstract_, so you'll need to implement it with the variable elimination\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(object):\n",
    "    \"\"\"\n",
    "    Represents a Bayesian network whose nodes are discrete random variables.\n",
    "    Note: we deal with nodes entirely based on their indices, only referring\n",
    "    to the RandomVariable objects when we need to access their distributions.\n",
    "    \"\"\"\n",
    "    def __init__(self, random_vars):\n",
    "        self.nodes = random_vars # list of RandomVariables\n",
    "        self.reset_rels()\n",
    "    def reset_rels(self):\n",
    "        self.edges = []\n",
    "        self.cond_probs = [{} for _ in self.nodes]\n",
    "        self.children = [[] for _ in self.nodes]\n",
    "        self.parents = [[] for _ in self.nodes]\n",
    "    def load_other_net(self, other):\n",
    "        \"Create a clone of another Bayes' net. Useful for comparing child classes.\"\n",
    "        self.nodes = list(other.nodes)\n",
    "        self.edges = list(other.edges)\n",
    "        self.cond_probs = list(other.cond_probs)\n",
    "        self.children = list(other.children)\n",
    "        self.parents = list(other.parents)\n",
    "    # list out the children of each node. this is enough for someone (you)\n",
    "    # to sketch the structure of the network if need be.\n",
    "    def __str__(self):\n",
    "        s = \"BayesNet\"\n",
    "        for i in range(len(self.nodes)):\n",
    "            s += \"\\nNode {0} children: {1}\".format(i, self.children[i])\n",
    "        return s\n",
    "    def add_edge(self, i, j):\n",
    "        \"\"\"Add edge from node i to node j.\"\"\"\n",
    "        self.edges.append((i, j))\n",
    "        self.children[i].append(j)\n",
    "        self.parents[j].append(i)\n",
    "    def update_cond_probs(self, i, mapping):\n",
    "        \"\"\"Set the conditional probabilities matrix for node i.\"\"\"\n",
    "        self.cond_probs[i] = mapping\n",
    "    def random_edges(self, style=\"ConstantBound\", param=1):\n",
    "        \"\"\"\n",
    "        Create a random set of edges for this BayesNet in one of two ways:\n",
    "        \n",
    "         1. \"ConstantBound\": the second argument passed to the function acts\n",
    "            as an upper bound on the number of arrows leaving any given node.\n",
    "            In the lecture notes, this is the structure that leads to polynomial\n",
    "            runtime of the variable elimination algorithm - O(N^param).\n",
    "         2. \"Proportion\": the second argument passed to the function represents\n",
    "            what fraction of edges should be included in the network. In other\n",
    "            words, for each potential edge i --> j, we include i --> j in the\n",
    "            network with probability param.\n",
    "        \n",
    "        In either case, we only include edges i --> j with i < j. This doesn't\n",
    "        really limit the scope of what we're doing here, because every directed\n",
    "        acyclic graph admits a topological ordering.\n",
    "        \"\"\"\n",
    "        self.reset_rels()\n",
    "        if style==\"ConstantBound\":\n",
    "            for i in range(len(self.nodes)):\n",
    "                num_children = min(len(self.nodes)-1 - i, max(int(np.random.uniform() * (param+1)), 1))\n",
    "                children = np.random.choice(list(range(i+1, len(self.nodes))), num_children, False)\n",
    "                for j in children:\n",
    "                    self.add_edge(i, j)\n",
    "        elif style==\"Proportion\":\n",
    "            for i in range(len(self.nodes)):\n",
    "                for j in range(i+1, len(self.nodes)):\n",
    "                    if np.random.uniform() < param:\n",
    "                        self.add_edge(i, j)\n",
    "    def random_cond_probs(self):\n",
    "        \"\"\"\n",
    "        Randomly generate conditional probability matrices for ALL nodes\n",
    "        in the network. Will run without error even if the network has no edges,\n",
    "        but random_edges should generally be called before this for random\n",
    "        network generation.\n",
    "        \"\"\"\n",
    "        self.cond_probs = [{} for _ in self.nodes]\n",
    "        for i in range(len(self.nodes)):\n",
    "            parents = self.parents[i]\n",
    "            all_outcomes = [self.nodes[parent].values for parent in parents]\n",
    "            conditions = list(itertools.product(*all_outcomes))\n",
    "            for cond in conditions:\n",
    "                self.cond_probs[i][cond] = self.nodes[i].random_clone()\n",
    "    def marginal_probability(vars_to_vals):\n",
    "        \"\"\"\n",
    "        ABSTRACT method that returns the probability of a given subset of\n",
    "        the random variables taking on a certain set of values.\n",
    "        \n",
    "        Example input: {1:\"asdf\", 2:6, 7:(6,5,4)}\n",
    "        Example output: 0.432\n",
    "        Example interpretation: The probability (based on all conditional\n",
    "                                probabilities making up the BayesNet) that\n",
    "                                variables 1, 2, and 7 will take the values\n",
    "                                \"asdf\", 6, and (6,5,4) (all hashable),\n",
    "                                respectively, is 0.432.\n",
    "        \"\"\"\n",
    "        assert False, \"Method marginal_probability is abstract and cannot be called\"\n",
    "    def conditional_probability(vars_to_vals_pred, vars_to_vals_cond):\n",
    "        \"\"\"\n",
    "        Return the conditional probability that a certain subset of the\n",
    "        random variables in this network will take on a certain tuple of\n",
    "        values given that another subset of the random variables takes on\n",
    "        a certain tuple of values. Note that this method calls marginal_probability,\n",
    "        so it only works correctly in child classes of BayesNet that have\n",
    "        implemented marginal_probability correctly.\n",
    "        \"\"\"\n",
    "        for key in vars_to_vals_pred:\n",
    "            if vars_to_vals_cond.get(key) is not None and vars_to_vals_cond[key] != vars_to_vals_pred[key]:\n",
    "                return 0\n",
    "        vars_to_vals_pred.update(vars_to_vals_cond)\n",
    "        denom = self.marginal_probability(vars_to_vals_cond)\n",
    "        if not denom:\n",
    "            return -1\n",
    "        numer = self.marginal_probability(vars_to_vals_pred)\n",
    "        return numer / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's one example of a child class of `BayesNet` implementing the `marginal_probability`\n",
    "method. This uses the brute force algorithm of calculating the joint distribution\n",
    "for every outcome comprising the event. The code is uncommented, because in industry\n",
    "you often have to deal with code that is insufficiently commented written by an engineer\n",
    "of limited or no availability (_we can attest_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BruteForceBayesNet(BayesNet):\n",
    "    \"\"\"\n",
    "    Implementation of the brute force algorithm for calculating marginal probabilities.\n",
    "    \"\"\"\n",
    "    def joint_distribution(self):\n",
    "        all_outcomes = [rv.values for rv in self.nodes]\n",
    "        conditions = list(itertools.product(*all_outcomes))\n",
    "        out = {cond:self.joint_distribution_eval(cond) for cond in conditions}\n",
    "        return out\n",
    "    def joint_distribution_eval(self, tup):\n",
    "        prob = 1.0\n",
    "        for i in range(len(self.nodes)):\n",
    "            parents = self.parents[i]\n",
    "            parent_vals = tuple([tup[p] for p in parents])\n",
    "            cond_rv = self.cond_probs[i][parent_vals]\n",
    "            prob *= cond_rv.probs_map[tup[i]]\n",
    "        return prob\n",
    "    def marginal_probability(self, vars_to_vals):\n",
    "        jd = self.joint_distribution()\n",
    "        p = 0\n",
    "        for tup in jd:\n",
    "            match = True\n",
    "            for i in vars_to_vals:\n",
    "                if tup[i] != vars_to_vals[i]:\n",
    "                    match = False\n",
    "            if match:\n",
    "                p += jd[tup]\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to walk you through implementing the variable elimination algorithm\n",
    "in the `VariableEliminationBayesNetClass` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We include these as a starting point for your implementation.\n",
    "\n",
    "Remember memoization from CS 61A? When we speak of \"precomputing\" the values\n",
    "of a function, the advantage we really want is the ability to only have to \n",
    "compute the value of the function at most once for a specific set of inputs.\n",
    "After that, we want to have constant-time evaluation. That's what this class\n",
    "does. Take a look at how it works; we encourage you to use it in your solution.\n",
    "\"\"\"\n",
    "class MemoizedFunction(object):\n",
    "    \"\"\"\n",
    "    Represents a function that caches all of the values it's calculated\n",
    "    so that repeat sets of arguments have constant-time evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, func):\n",
    "        # we use this to derive keys for the cache\n",
    "        self.args = args\n",
    "        self.func = func\n",
    "        # store previously calculated values\n",
    "        self.cache = {}\n",
    "    def evaluate(self, param_map):\n",
    "        \"\"\" Evaluate this function given a dictionary mapping argument names to values. \"\"\"\n",
    "        # derive a tuple of parameters from input dictionary\n",
    "        params = tuple([param_map[arg] for arg in self.args])\n",
    "        # if we haven't seen these inputs before, update the cache\n",
    "        if params not in self.cache:\n",
    "            self.cache[params] = self.func(param_map)\n",
    "        # now no matter what, our cache will contain an entry corresponding to the given inputs\n",
    "        return self.cache[params]\n",
    "    def __repr__(self):\n",
    "        # use for debugging if convenient\n",
    "        return self.func.__name__ + \"(\" + \", \".join([str(arg) for arg in self.args]) + \")\"\n",
    "\n",
    "def prod(lyst):\n",
    "    \"\"\" Return the product of the elements of a list, or 1 if the list is empty. \"\"\"\n",
    "    p = 1\n",
    "    for e in lyst:\n",
    "        p *= e\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for manipulating MemoizedFunction objects\n",
    "def reduce(mf, some_args):\n",
    "    \"\"\"\n",
    "    Given a MemoizedFunction and a partial set of inputs, return a new\n",
    "    MemoizedFunction that takes only the remaining inputs and returns the\n",
    "    original function evaluated on all inputs.\n",
    "    \n",
    "    Example:\n",
    "      mf1 = MemoizedFunction(['a', 'b', 'c', 'd'], lambda d:sum(d.values()))\n",
    "      mf1.evaluate({'a':1, 'b':2, 'c':3, 'd':4})  # 1 + 2 + 3 + 4 = 10\n",
    "      \n",
    "      mf2 = reduce(mf1, {'a': 5, 'c': 9})\n",
    "      mf2.args  # ['b', 'd']\n",
    "      mf2.evaluate({'b':2', 'd':4})  # 5 + 2 + 9 + 4 = 20\n",
    "    \"\"\"\n",
    "    # start with the arguments of mf, and remove the arguments passed in some_args\n",
    "    newargs = ### TODO ###\n",
    "    def newfunc(param_map):\n",
    "        ### TODO ###\n",
    "        pass\n",
    "    return MemoizedFunction(newargs, newfunc)\n",
    "\n",
    "def merge_functions(mfs, comb):\n",
    "    \"\"\"\n",
    "    Given a list MFS of MemoizedFunctions and a function COMB\n",
    "    which takes a list as its sole argument, return a MemoizedFunction\n",
    "    whose arguments are all arguments of the MemoizedFunctions in MFS\n",
    "    and whose output is COMB applied to the outputs of each element of MFS\n",
    "    \n",
    "    Example:\n",
    "      # all of these return the sum of their arguments\n",
    "      mf1 = MemoizedFunction(['a', 'b', 'c', 'd'], lambda d:sum(d.values()))\n",
    "      mf2 = MemoizedFunction(['c', 'd', 'e'], lambda d:sum(d.values()))\n",
    "      mf3 = MemoizedFunction(['b', 'd'], lambda d:sum(d.values()))\n",
    "      \n",
    "      mf_prod = merge_functions([mf1, mf2, mf3], prod) # prod: product of elements of list\n",
    "      mf_prod.args  # ['a', 'b', 'c', 'd', 'e']\n",
    "      mf_prod.evaluate({'a':1, 'b':2, 'c':3, 'd':4, 'e':5})  # (1+2+3+4)*(3+4+5)*(2+4) = 720\n",
    "    \"\"\"\n",
    "    # the set data structure is good for unions\n",
    "    newargs_set = ### TODO ###\n",
    "    def newfunc(param_map):\n",
    "        func_vals = []\n",
    "        # evaluate each constituent function\n",
    "        ### TODO ###\n",
    "        # and return the combined result\n",
    "        return comb(func_vals)\n",
    "    return MemoizedFunction(newargs, newfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableEliminationBayesNet(BayesNet):\n",
    "    \"\"\"\n",
    "    A Bayesian network which evaluates marginal probabilities\n",
    "    via the variable elimination algorithm.\n",
    "    \"\"\"\n",
    "    def access_cond_prob(self, var, val, cond_vars_to_vals):\n",
    "        \"\"\" A function to access conditional probabilities in this BayesNet. \"\"\"\n",
    "        parents = self.parents[var]\n",
    "        cond_vals_tup = tuple([cond_vars_to_vals[p] for p in parents])\n",
    "        return self.cond_probs[var][cond_vals_tup].probs_map[val]\n",
    "    def probability_access_function(self, i):\n",
    "        \"\"\" Return a modified form of access_cond_prob which takes a dictionary as input. \"\"\"\n",
    "        def function(param_map):\n",
    "            return self.access_cond_prob(i, param_map[i], param_map)\n",
    "        return function\n",
    "    def marginal_probability(self, vars_to_vals):\n",
    "        \"\"\"\n",
    "        Given some subset of the variables in this BayesNet and their values,\n",
    "        determine the probability that all of the passed variables take their\n",
    "        corresponding values.\n",
    "        \"\"\"\n",
    "        # the variables not passed in vars_to_vals\n",
    "        other_vars = [i for i in range(len(self.nodes)) if i not in vars_to_vals]\n",
    "        \n",
    "        # for each variable, create a MemoizedFunction accessing the\n",
    "        # conditional probability matrix for that variable.\n",
    "        funcs = []\n",
    "        for i in range(len(self.nodes)):\n",
    "            parents = self.parents[i]\n",
    "            function = self.probability_access_function(i)\n",
    "            # the conditional probability depends on the variable and its parents\n",
    "            mf = MemoizedFunction([i] + parents, function)\n",
    "            # but we already know the values of some of the variables\n",
    "            rmf = ### TODO ###\n",
    "            funcs.append(rmf)\n",
    "        self.funcs = funcs\n",
    "        \n",
    "        # we choose to eliminate the other variables in order of how many\n",
    "        # arguments their associated function has. There are several heuristics\n",
    "        # for choosing which variable to eliminate, but the problem of determining\n",
    "        # which order is best is actually NP-hard. Feel free to modify as you'd like.\n",
    "        other_vars = sorted(other_vars, key=lambda i:len(funcs[i].args))\n",
    "        \n",
    "        # we need to \"eliminate\" each of the other variables by summing over all possible values. \n",
    "        for i in other_vars:\n",
    "            self.eliminate_variable(i)\n",
    "        for func in self.funcs:\n",
    "            assert func.args == [], \"Failed to eliminate all variables!\"\n",
    "            \n",
    "        # Think about what self.funcs looks like after you eliminate all of the variables\n",
    "        # Hint: what if the network has no arrows?\n",
    "        return ### TODO ###\n",
    "    def all_associated_funcs(self, var):\n",
    "        \"\"\"\n",
    "        Return all functions with this varaible as an argument.\n",
    "        We need to know this so that we can multiply them together and\n",
    "        precalculate all values of the product (look back at the lecture\n",
    "        note if you're not sure about this).\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        ### TODO loop over self.funcs ###\n",
    "        return output\n",
    "    def eliminate_variable(self, var):\n",
    "        \"\"\"\n",
    "        Eliminate (at least) one variable VAR from our expression \n",
    "        for the marginal probability.\n",
    "        \n",
    "        Steps:\n",
    "          1. Identify all functions that have VAR as an argument.\n",
    "          2. Join these functions into one function whose arguments are\n",
    "             all of the arguments of the constituent functions (how do\n",
    "             we join them together?)\n",
    "             Hint: use merge_functions with a proper choice of combining \n",
    "                   function.\n",
    "          3. Sum this joined function over all possible values of VAR.\n",
    "             Hint: use reduce a bunch of times to yield a list of reduced\n",
    "                   functions, and use merge_functions to combine them.\n",
    "        \"\"\"\n",
    "        # identify functions with VAR as an argument\n",
    "        func_list = ### TODO ###\n",
    "        \n",
    "        ### TODO remove those functions from self.funcs, since we'll be combining them ###\n",
    "        \n",
    "        ### TODO step 2 (see docstring) ###\n",
    "        \n",
    "        ### TODO step 3 (see docstring) ###\n",
    "        \n",
    "        ### TODO add the combined function back into self.funcs ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Speed-up\n",
    "Now that you've implemented the variable elimination algorithm, run the scripts below to plot the speed of variable elimination versus the speed of the brute force implementation. The `test` method takes the following arguments\n",
    "  - `num_vars`: the number of variables with which to build the `BayesNet`(s)\n",
    "  - `num_vals`: the number of possible values for each variable\n",
    "  - `edges_type`: how the `BayesNet`(s) should be generated: `\"ConstantBound\"` or `\"Proportion\"`\n",
    "  - `edges_param`: the associated parameter to `edges_type` (see above)\n",
    "  - `num_eval_vars`: the number of variable-value pairs to pass to `marginal_probability`\n",
    "  - `trials`: the number of trials to run\n",
    "  - `net_structure`: to use a specific set of edges, pass a `BayesNet`, and its edges will get copied and used in the test\n",
    "  - `both`: True iff the test should assess both variable elimination _and_ brute force (you'd turn this off if you expect brute force to be especially slow and want to analyze the asymptotic complexity of variable elimination)\n",
    "  - `show_progress`: if True, displays a progress bar iPython widget\n",
    "\n",
    "Given these arguments, `test` runs `trials` trials, where in each trial it calculates a single marginal probability with a randomly generated set of variables and associated values. Note that a _single_ Bayesian network is used throughout the test, so depending on the network, `test` could show varying behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(num_vars, num_vals, edges_type, edges_param, num_eval_vars, trials, net_structure=None, both=True, show_progress=True):\n",
    "    rvs = []\n",
    "    for i in range(num_vars):\n",
    "        rvs.append(RandomVariable(list(range(num_vals))))\n",
    "    bfbn = BruteForceBayesNet(rvs)\n",
    "    bfbn.random_edges(edges_type, edges_param)\n",
    "    if net_structure:\n",
    "        bfbn.edges = net_structure.edges\n",
    "        bfbn.children = net_structure.children\n",
    "        bfbn.parents = net_structure.parents\n",
    "    bfbn.random_cond_probs()\n",
    "    vebn = VariableEliminationBayesNet([])\n",
    "    vebn.load_other_net(bfbn)\n",
    "    total_bf = 0\n",
    "    total_ve = 0\n",
    "    if show_progress:\n",
    "        progress_bar = IntProgress(min=0, max=trials)\n",
    "        display(progress_bar)\n",
    "    for _ in range(trials):\n",
    "        vars_to_check = np.random.choice(list(range(num_vars)), num_eval_vars, replace=False)\n",
    "        vars_to_vals = {}\n",
    "        for v in vars_to_check:\n",
    "            vars_to_vals[v] = np.random.choice(rvs[v].values)\n",
    "        start = time.time()\n",
    "        ve_val = vebn.marginal_probability(vars_to_vals)\n",
    "        end = time.time()\n",
    "        total_ve += end-start\n",
    "        if both:\n",
    "            start = time.time()\n",
    "            bf_val = bfbn.marginal_probability(vars_to_vals)\n",
    "            end = time.time()\n",
    "            total_bf += end-start\n",
    "            assert round(abs(ve_val - bf_val), 5) == 0, \"Variable elimination value \" + str(ve_val) + \"does not match brute force value \" + str(bf_val) + \"\\n\" + str(bfbn)\n",
    "        if show_progress:\n",
    "            progress_bar.value += 1\n",
    "    if show_progress:\n",
    "        progress_bar.layout.display = 'none'\n",
    "    return total_bf/trials, total_ve/trials, bfbn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `test_constant_bound` accepts similar parameters to `test`. The purpose of this method is to examine the asymptotic time complexity of variable elimination for a Bayesian network with nodes of bounded degree (i.e. every node has at most $k$ arrows coming out of it, for some $k$). It will vary the number of values that each random variable in the network can take on, testing each number of values from `low` (inclusive) to `high` (exclusive). It uses a single Bayesian network structure (i.e. the same set of arrows) for all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_constant_bound(num_vars, low, high, edges_param, trials_per, both=False, show_progress=False):\n",
    "    l = []\n",
    "    xvals = np.array(list(range(low, high)))\n",
    "    struct = None\n",
    "    for i in xvals:\n",
    "        bft, vet, struct = test(num_vars, i, \"ConstantBound\", edges_param, 1, trials_per, net_structure=struct, both=both, show_progress=show_progress)\n",
    "        l.append((bft, vet))\n",
    "        \n",
    "    bf_times = np.array([t[0] for t in l])\n",
    "    ve_times = np.array([t[1] for t in l])\n",
    "    line_bf, = plt.plot(xvals, bf_times, label=\"Brute force\")\n",
    "    line_ve, = plt.plot(xvals, ve_times, label=\"Variable elimination\")\n",
    "    power_approx = np.power(xvals, edges_param)\n",
    "    ratio = ve_times / power_approx\n",
    "    ratio = ratio / np.mean(ratio) * np.mean(ve_times)\n",
    "    line_rat, = plt.plot(xvals, ratio, label=\"Ratio\")\n",
    "    plt.legend(handles=[line_bf, line_ve, line_rat])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "First, run this line of code to compare the brute force algorithm and the variable elimination algorithm. Broadly, what do you notice? Ignore the ratio line for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_constant_bound(5, 2, 7, 2, 100, both=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "Run the following line of code to examine how the time it takes to run variable elimination increases with the number of values taken by each random variable. The Ratio in the output plot shows the ratio between the variable elimination run time and $N^K$, where $N$ is the number of values that each random variable can take on, and $K$ is the number of arrows going out from each node. What do you notice? Does this agree with what we derived in the lecture note?\n",
    "\n",
    "Note: try running this line several times and messing around with the parameters (larger values are generally conducive to more theoretically correct results). The behavior of this function can depend on the Bayesian network structure being used (which is randomly generated with each run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_constant_bound(5, 2, 12, 2, 100, both=False, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `test_proportion` accepts similar parameters to `test`. The purpose of this method is to examine the asymptotic time complexity of variable elimination for a Bayesian network with some fixed fraction of the arrows present (i.e. each arrow is present with probability $\\alpha \\in (0,1)$ - this is not actually how it works as we have implemented it, but it's close enough). It will vary the number of values that each random variable in the network can take on, testing each number of values from `low` (inclusive) to `high` (exclusive). It uses a single Bayesian network structure (i.e. the same set of arrows) for all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_proportion(num_vars, low, high, edges_param, trials_per, both=False, show_progress=False):\n",
    "    l = []\n",
    "    xvals = np.array(list(range(low, high)))\n",
    "    struct = None\n",
    "    for i in xvals:\n",
    "        bft, vet, struct = test(num_vars, i, \"Proportion\", edges_param, 1, trials_per, both=both, show_progress=show_progress)\n",
    "        l.append((bft, vet))\n",
    "    \n",
    "    bf_times = np.array([t[0] for t in l])\n",
    "    ve_times = np.array([t[1] for t in l])\n",
    "    line_bf, = plt.plot(xvals, bf_times, label=\"Brute force\")\n",
    "    line_ve, = plt.plot(xvals, ve_times, label=\"Variable elimination\")\n",
    "    log_approx = np.log(ve_times)\n",
    "    ratio = log_approx / xvals\n",
    "    ratio = ratio / np.mean(ratio) * np.mean(ve_times)\n",
    "    line_rat, = plt.plot(xvals, ratio, label=\"Ratio\")\n",
    "    plt.legend(handles=[line_bf, line_ve, line_rat])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "Run the following line of code to visualize how the running time of variable elimination increases with the number of values of each variable in the case that some constant proportion of the edges in the Bayesian network are present. In this case, the Ratio in the output plot is the ratio between the natural logarithm of the variable elimination run time and the number of values taken on by each variable. What do you notice? Does this agree with what we derived in the lecture note?\n",
    "\n",
    "Note: try running this line several times and messing around with the parameters (larger values are generally conducive to more theoretically correct results). The behavior of this function can depend on the Bayesian network structure being used (which is randomly generated with each run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proportion(5, 2, 12, 0.4, 100, both=False, show_progress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
